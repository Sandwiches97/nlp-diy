{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# The Dataset for Pretraining BERT\n",
    ":label:`sec_bert-dataset`\n",
    "\n",
    "总结\n",
    "\n",
    "\n",
    "* 与PTB数据集相比，WikiText-2数据集保留了原来的**标点符号**、**大小写**和**数字**，并且比PTB数据集大了两倍多。\n",
    "* 我们可以任意访问从WikiText-2语料库中的一对句子生成的预训练（遮蔽语言模型和下一句预测）样本。\n",
    "* 训练bert时，两个预训练任务是同时进行的，所以我们需要把bert输入改成（NSP+MLM）输入\n",
    "  * 先转NSP（50% 是下一句，50%概率是随机句子）\n",
    "  * 再转MLM\n",
    "\n",
    "To pretrain the BERT model as implemented in [8_bert.md](8_bert.md), we need to **generate the dataset** in the ideal format to facilitate the two pretraining tasks: **masked language modeling** and **next sentence prediction**.\n",
    "\n",
    "- On one hand, the **original BERT model** is pretrained on the concatenation of two huge corpora BookCorpus and English Wikipedia (see [8_bert.md section 5](8_bert.md), making it hard to run for most readers of this book.\n",
    "- On the other hand, the off-the-shelf （现成的） pretrained BERT model **may not fit** for applications from specific domains like medicine.\n",
    "- Thus, it is getting popular to pretrain BERT on **a customized dataset**. 自己训练 预训练模型~\n",
    "\n",
    "To facilitate the demonstration of BERT pretraining, we use a smaller corpus `WikiText-2` [[Merity et al., 2016]](https://d2l.ai/chapter_references/zreferences.html#merity-xiong-bradbury-ea-2016).\n",
    "\n",
    "Comparing with the `PTB dataset` used for pretraining `word2vec` in [Section 14.3](https://d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html#sec-word2vec-data), `WikiText-2`\n",
    "\n",
    "- (i) retains the original punctuation (保留了原始的标点符号), making it suitable for **next sentence prediction**;\n",
    "- (ii) retains the original case （原始的大小写） and numbers （数字）;\n",
    "- (iii) is over twice larger.（大了一倍以上）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\FangC\\SourceCode\\pytorch\\NLP_diy\\bert_diy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from d2l_en.pytorch.d2l import torch as d2l\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "In the `WikiText-2` dataset, each line represents a paragraph where space is inserted between any punctuation and its preceding token. Paragraphs with at least two sentences are retained （保留至少有两句话的段落）. To split sentences, we only use the period (句号) as the delimiter （分隔符） for simplicity. We leave discussions of more complex sentence splitting techniques in the exercises （练习题） at the end of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "#@save\n",
    "def _read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # Uppercase letters are converted to lowercase ones\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## 9.1. Defining Helper Functions for Pretraining Tasks\n",
    "\n",
    "In the following, we begin by implementing helper functions for the two BERT pretraining tasks: next sentence prediction and masked language modeling. These helper functions will be invoked later when **transforming** the raw text corpus **into** the dataset of the ideal format to pretrain BERT.\n",
    "\n",
    "### 9.1.1 Generating the Next Sentence Prediction Task\n",
    "\n",
    "According to descriptions of [8_bert.md Section 8.5.2](8_bert.md), the `_get_next_sentence` function generates a training example for the binary classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_next_sentence(sentence:list, next_sentence:list, paragraphs:list):\n",
    "    \"\"\"\n",
    "    the `_get_next_sentence` generates a training example for the binary classification task.\n",
    "    :param sentence:\n",
    "    :param next_sentence:\n",
    "    :param paragraphs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # 'paragraphs' is a list of lists of lists (三重列表的嵌套)\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "The following function generates training examples for next sentence prediction from the input `paragraph` by invoking the `_get_next_sentence` function. Here `paragraph` is a list of sentences, where each sentence is a list of tokens. The argument `max_len` specifies the maximum length of a BERT input sequence during pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "### 9.1.2 Generating the Masked Language Modeling (MLM) Task\n",
    "\n",
    ":label:`subsec_prepare_mlm_data`\n",
    "\n",
    "In order to generate training examples for the MLM task from a BERT input sequence, we define the following `_replace_mlm_tokens` function.\n",
    "\n",
    "In its inputs,\n",
    "\n",
    "- `tokens` is a list of tokens representing a BERT input sequence,\n",
    "- `candidate_pred_positions` is a list of token indices of the BERT input sequence **excluding those of special tokens** (special tokens are not predicted in the MLM task), and\n",
    "- `num_mlm_preds` indicates the number of predictions (recall 15% random tokens to predict).\n",
    "\n",
    "Following the definition of the MLM task in [8_bert Section 8.5.1](8_bert.md), at each prediction position, the input may be replaced by a special `“<mask>”` token or a random token, or remain unchanged. In the end, the function returns the input tokens after possible replacement, the token indices where predictions take place and labels for these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _replace_mlm_tokens(tokens:list, candidate_pred_positions:list, \n",
    "                        num_mlm_preds:int, vocab)->Tuple[list, List[tuple]]:\n",
    "    \"\"\"\n",
    "\n",
    "    :param tokens: BERT input sequence\n",
    "    :param candidate_pred_positions: 预测的位置\n",
    "    :param num_mlm_preds: 预测的数量\n",
    "    :param vocab: 语料库 class 'C2_Text_Preprocessing.Vocab'\n",
    "    :return:  a tuple: (mlm_input_tokens, pred_positions_and_labels)\n",
    "    \"\"\"\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    # Shuffle for getting 15% random tokens for prediction in the MLM task\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None # 初始化变量\n",
    "        # 80% of the time: replace the word with the \"<mask>\" token\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = \"<mask>\"\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "            else:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "By invoking the aforementioned `_replace_mlm_tokens` function,\n",
    "the following function takes a BERT input sequence (`tokens`)\n",
    "as an input and returns indices of the input tokens\n",
    "(after possible token replacement as described in :numref:`subsec_mlm`),\n",
    "the token indices where predictions take place,\n",
    "and label indices for these predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "    # `tokens` is a list of strings\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Special tokens are not predicted in the masked language modeling\n",
    "        # task\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 15% of random tokens are predicted in the masked language modeling task\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
    "                                       key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Transforming Text into the Pretraining Dataset\n",
    "\n",
    "Now we are almost ready to customize a `Dataset` class for pretraining BERT. 接下来就是要生成一个Dataset class了。\n",
    "\n",
    "Before that, we still need to define a helper function `_pad_bert_inputs` **to append the special `“mask”` tokens to the inputs**.\n",
    "\n",
    "Its argument `examples`contain the outputs **from** the helper functions `_get_nsp_data_from_paragraph` **and** `_get_mlm_data_from_tokens` for the two pretraining tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _pad_bert_inputs(examples:List[tuple], max_len:int, vocab)->Tuple[list,list, list, list, list, list, list]:\n",
    "    \"\"\"  填 0 操作\n",
    "\n",
    "    :param examples: MLM预训练的输入信息\n",
    "    :param max_len: 最大长度\n",
    "    :param vocab: class Vocab\n",
    "    :return: 处理后的符合bert输入的  MLM预训练任务\n",
    "    \"\"\"\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        all_token_ids.append(np.array(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype='int32'))\n",
    "        all_segments.append(np.array(segments + [0] * (max_len - len(segments)), dtype='int32'))\n",
    "        # `valid_len` excludes count of `<pad>` tokens\n",
    "        valid_lens.append(np.array(len(token_ids), dtype='int32'))\n",
    "        all_pred_positions.append(np.array(pred_positions + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype='int32'))\n",
    "        # Predictions of padded tokens will be filtered out in the loss via multiplication of 0 weights\n",
    "        all_mlm_weights.append(\n",
    "            np.array([1.0] * len(mlm_pred_label_ids) + [0.0] *(max_num_mlm_preds - len(pred_positions)), dtype='float32')\n",
    "        )\n",
    "        all_mlm_labels.append(\n",
    "            np.array(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype='int32'    )\n",
    "        )\n",
    "        nsp_labels.append(np.array(is_next))\n",
    "    return (all_token_ids, all_segments, valid_lens,\n",
    "           all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "Putting the helper functions for\n",
    "generating training examples of the two pretraining tasks,\n",
    "and the helper function for padding inputs together,\n",
    "we customize the following `_WikiTextDataset` class as the WikiText-2 dataset for pretraining BERT.\n",
    "By implementing the `__getitem__ `function,\n",
    "we can arbitrarily access the pretraining (masked language modeling and next sentence prediction) examples \n",
    "generated from a pair of sentences from the WikiText-2 corpus.\n",
    "\n",
    "The original BERT model uses WordPiece embeddings whose vocabulary size is 30000 :cite:`Wu.Schuster.Chen.ea.2016`.\n",
    "The tokenization method of WordPiece is a slight modification of\n",
    "the original byte pair encoding algorithm in :numref:`subsec_Byte_Pair_Encoding`.\n",
    "For simplicity, we use the `d2l.tokenize` function for tokenization.\n",
    "Infrequent tokens that appear less than five times are filtered out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # Input `paragraphs[i]` is a list of sentence **strings** representing a paragraph;\n",
    "        # while output `paragraphs[i]` is a list of sentences representing a paragraph,\n",
    "        # where each sentence is a list of **tokens**\n",
    "        paragraphs = [d2l.tokenize(\n",
    "            paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs\n",
    "                     for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
    "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # Get data for the next sentence prediction task\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "        # Get data for the masked language model task\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                      + (segments, is_next))\n",
    "                     for tokens, segments, is_next in examples]\n",
    "        # Pad inputs\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n",
    "            examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "By using the `_read_wiki` function and the `_WikiTextDataset` class,\n",
    "we define the following `load_data_wiki` to download and WikiText-2 dataset\n",
    "and generate pretraining examples from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"Load the WikiText-2 dataset.\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "Setting the batch size to 512 and the maximum length of a BERT input sequence to be 64,\n",
    "we print out the shapes of a minibatch of BERT pretraining examples.\n",
    "Note that in each BERT input sequence,\n",
    "$10$ ($64 \\times 0.15$) positions are predicted for the masked language modeling task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
    "     mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "In the end, let us take a look at the vocabulary size.\n",
    "Even after filtering out infrequent tokens,\n",
    "it is still over twice larger than that of the PTB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Comparing with the PTB dataset, the WikiText-2 dateset retains the original punctuation, case and numbers, and is over twice larger.\n",
    "* We can arbitrarily access the pretraining (masked language modeling and next sentence prediction) examples generated from a pair of sentences from the WikiText-2 corpus.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. For simplicity, the period is used as the only delimiter for splitting sentences. Try other sentence splitting techniques, such as the spaCy and NLTK. Take NLTK as an example. You need to install NLTK first: `pip install nltk`. In the code, first `import nltk`. Then, download the Punkt sentence tokenizer: `nltk.download('punkt')`. To split sentences such as `sentences = 'This is great ! Why not ?'`, invoking `nltk.tokenize.sent_tokenize(sentences)` will return a list of two sentence strings: `['This is great !', 'Why not ?']`.\n",
    "1. What is the vocabulary size if we do not filter out any infrequent token?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1496)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
