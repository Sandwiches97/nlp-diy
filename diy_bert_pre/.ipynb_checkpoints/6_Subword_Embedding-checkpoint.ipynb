{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f91e243",
   "metadata": {},
   "source": [
    "# 6. Subword Embedding\n",
    "\n",
    "In English, words such as “helps”, “helped”, and “helping” are inflected forms of **the same word** “help”. The relationship between “dog” and “dogs” is the **same as** that between “cat” and “cats”, and the relationship between “boy” and “boyfriend” is the **same as** that between “girl” and “girlfriend”. In other languages such as French and Spanish, many verbs have over 40 inflected forms, while in Finnish, a noun may have up to 15 cases. In linguistics, morphology studies word formation and word relationships. \n",
    "\n",
    "However, the internal structure of words was neither explored in `word2vec` nor in `GloVe`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69decaf8",
   "metadata": {},
   "source": [
    "## 6.1. The fastText Model\n",
    "\n",
    "Recall how words are represented in `word2vec`. In both the `skip-gram` model and the continuous `bag-of-words` model, different inflected forms of the same word are directly represented by **different vectors without shared parameters**. To use morphological information, the `fastText` model proposed a **subword embedding approach**, where a subword is a character $n$-gram [Bojanowski et al., 2017]. **Instead of** learning word-level vector representations, `fastText` can be considered as the *subword-level skip-gram*, where each center word is represented by the sum of its subword vectors.\n",
    "\n",
    "Let us illustrate how to obtain subwords for each center word in fastText using the word “where”. \n",
    "1. First, add special characters “<” and “>” at the **beginning** and **end** of the word to **distinguish prefixes and suffixes** from other subwords. \n",
    "2. Then, extract character $n$-grams from the word. \n",
    "\n",
    "For example, when $n=3$, we obtain all subwords of length 3: “<wh”, “whe”, “her”, “ere”, “re>”, and the special subword “<where>”.\n",
    "\n",
    "In fastText, for any word $w$, denote by $\\mathcal{G}_w$ the union of all its subwords of length between 3 and 6 and its special subword. The vocabulary is the union of the subwords of all words. Letting $z_g$ be the vector of subword $g$ in the dictionary, the vector $v_w$ for word $w$ as a center word in the `skip-gram` model is the sum of its subword vectors:\n",
    "$$\n",
    "v_w = \\sum_{g\\in \\mathcal{g}_w}z_g\n",
    "$$\n",
    " \n",
    "The rest of fastText is the same as the `skip-gram` model. Compared with the `skip-gram` model, the vocabulary in `fastText` is larger, resulting in **more model parameters**. Besides, to calculate the representation of a word, all its subword vectors have to be summed, leading to higher computational complexity. However, thanks to shared parameters from subwords among words with similar structures, rare words and even out-of-vocabulary words may obtain better vector representations in fastText."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55925ff6",
   "metadata": {},
   "source": [
    "## 6.2. Byte Pair Encoding\n",
    "\n",
    "In fastText, all the extracted subwords have to be of the *specified lengths*, such as 3 to 6, thus the vocabulary size cannot be predefined. To allow for variable-length subwords in a fixed-size vocabulary, we can apply a compression algorithm called **byte pair encoding (BPE)** to extract subwords [[Sennrich et al., 2015]](https://d2l.ai/chapter_references/zreferences.html#sennrich-haddow-birch-2015).\n",
    "\n",
    "**BPE** performs a statistical analysis of the training dataset to discover common symbols within a word, such as consecutive characters of arbitrary length. Starting from symbols of length 1, **BPE** iteratively merges the most frequent pair of consecutive symbols to produce new longer symbols. Note that for efficiency, pairs crossing word boundaries are not considered. **In the end**, we can use such symbols as subwords to *segment words*. **BPE** and its variants has been used for input representations in popular NLP pretraining models such as GPT-2 [[Radford et al., 2019]](https://d2l.ai/chapter_references/zreferences.html#radford-wu-child-ea-2019) and RoBERTa [[Liu et al., 2019]](https://d2l.ai/chapter_references/zreferences.html#liu-ott-goyal-ea-2019). In the following, we will illustrate **how BPE works**.\n",
    "\n",
    "First, we **initialize** the vocabulary of symbols as all the English lowercase （小写） characters, a special end-of-word symbol `'_'` （用来分割单词）, and a special unknown symbol `'[UNK]'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d468365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "           '_', '[UNK]']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88b4a1",
   "metadata": {},
   "source": [
    "Since we do not consider symbol pairs that cross boundaries of words, we only need a dictionary `raw_token_freqs` that **maps** words **to** their *frequencies* (number of occurrences) in a dataset. Note that the special symbol `'_'` is appended to each word so that we can easily **recover** a word sequence (e.g., “a taller man” _用来分割单词，单词切片直接用空格分割) from a sequence of output symbols ( e.g., “a_ tall er_ man”). Since we start the merging process from a vocabulary of only single characters and special symbols, space is **inserted between** every pair of consecutive characters within each word (keys of the dictionary `token_freqs`). In other words, space is the delimiter between symbols within a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffabc2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_token_freqs = {\"fast_\": 4, \"faster_\": 3, \"tall_\": 5, \"taller_\": 4}\n",
    "token_freqs = {}\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "    \n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838bbce",
   "metadata": {},
   "source": [
    "We define the fpllowing `get_amx_freq_pair` function that returns **the most frequent pair** of consecutive symbols within a word, where words come from keys of the input dictionary `token_freqs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02f56b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_freq_pair (token_freqs):\n",
    "    \"\"\" 返回频次最高的连续符号对\n",
    "\n",
    "    :param token_freqs:  input dictionary, \n",
    "        such as {'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}\n",
    "    :return: the most frequent pair of consecutive symbols within a word\n",
    "        such as ('t a l l _', 5)\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Key of ' pairs ' is a tuple of two consecutive symbols\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return max(pairs, key=pairs.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46f8d9",
   "metadata": {},
   "source": [
    "As a greedy approach based on frequency of consecutive symbols, **BPE** will use the following `merge_symbols` function to merge the most frequent pair of consecutive symbols to produce new symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbe5c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    ''' merge the most frequent pair of consecutive symbols to produce new symbols\n",
    "\n",
    "    :param max_freq_pair:  the most frequent pair of consecutive symbols within a word\n",
    "        for example, ('t', 'a')\n",
    "    :param token_freqs:  the input dictionary,\n",
    "        such as: {'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}\n",
    "    :param symbols: alphabet\n",
    "        such as: {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "           '_', '[UNK]'}\n",
    "    :return: the new dictionary, 新加入了最高频率的字母组合\n",
    "    '''\n",
    "    symbols.append(''.join(max_freq_pair)) # 字母表中添加新元素 ,例如”ab“\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                                  ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbf975",
   "metadata": {},
   "source": [
    "Now we iteratively perform the **BPE** algorithm over the keys of the dictionary `token_freqs`.\n",
    "\n",
    "In the first iteration, the most frequent pair of consecutive symbols are `'t'` and `'a'`, thus **BPE** merges them to produce a new symbol `'ta'`.\n",
    "\n",
    "In the second iteration, **BPE** continues to merge `'ta'` and `'l'` to result in another new symbol `'tal'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4482ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge #1: ('t', 'a')\n",
      "{'f a s t _': 4, 'f a s t e r _': 3, 'ta l l _': 5, 'ta l l e r _': 4}\n",
      "merge #2: ('ta', 'l')\n",
      "{'f a s t _': 4, 'f a s t e r _': 3, 'tal l _': 5, 'tal l e r _': 4}\n",
      "merge #3: ('tal', 'l')\n",
      "{'f a s t _': 4, 'f a s t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "merge #4: ('f', 'a')\n",
      "{'fa s t _': 4, 'fa s t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "merge #5: ('fa', 's')\n",
      "{'fas t _': 4, 'fas t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "merge #6: ('fas', 't')\n",
      "{'fast _': 4, 'fast e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "merge #7: ('e', 'r')\n",
      "{'fast _': 4, 'fast er _': 3, 'tall _': 5, 'tall er _': 4}\n",
      "merge #8: ('er', '_')\n",
      "{'fast _': 4, 'fast er_': 3, 'tall _': 5, 'tall er_': 4}\n",
      "merge #9: ('tall', '_')\n",
      "{'fast _': 4, 'fast er_': 3, 'tall_': 5, 'tall er_': 4}\n",
      "merge #10: ('fast', '_')\n",
      "{'fast_': 4, 'fast er_': 3, 'tall_': 5, 'tall er_': 4}\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f'merge #{i + 1}:', max_freq_pair)\n",
    "    print(token_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c8be5",
   "metadata": {},
   "source": [
    "After 10 iterations of **BPE**, we can see that list ` symbols ` now contains 10 more symbols that are iteratively merged from **other symbols**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a599e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_'] \n",
      " the size of symbol is: 38\n"
     ]
    }
   ],
   "source": [
    "print(symbols, '\\n', f'the size of symbol is: {len(symbols)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a4d11",
   "metadata": {},
   "source": [
    "\n",
    "Note that the result of **BPE** depends on the dataset being used. We can also use the subwords learned from one dataset to segment words of another dataset. As a greedy approach, the following `segment_BPE` function tries to break words into the longest possible subwords from the input argument `symbols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcd95d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_BPE(tokens, symbols):\n",
    "    \"\"\"to break words into the longest possible subwords from the input argument symbols.\n",
    "         \n",
    "    :param tokens:  单词本\n",
    "    :param symbols:  alphabet\n",
    "    :return:   the longest possible subwords from the input argument symbols.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for token in tokens:\n",
    "        start, end = 0, len(token)\n",
    "        cur_output = []\n",
    "        # Segment token with the longest possible subwords from symbols\n",
    "        while start < len(token) and start < end:  # 双指针找最大不重复匹配字符串\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9573651",
   "metadata": {},
   "source": [
    "In the following, we use the subwords in list `symbols`, which is learned from the aforementioned dataset, to segment ` tokens` that represent another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aba0a864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tall e s t _', 'fa t t er_']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['tallest_', 'fatter_']\n",
    "print(segment_BPE(tokens, symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3872d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
