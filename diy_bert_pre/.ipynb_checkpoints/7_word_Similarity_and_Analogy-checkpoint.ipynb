{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e3bc11",
   "metadata": {},
   "source": [
    "# 7. Word Similarity and Analogy\n",
    "\n",
    "In Section 4, we trained a 1`word2vec` model on a small dataset, and applied it to find semantically similar words for an input word. In practice, word vectors that are pretrained on large corpora can be applied to downstream natural language processing tasks, which will be covered later in Section 5. \n",
    "\n",
    "To demonstrate semantics of pretrained word vectors from large corpora in a straightforward way, let us apply them in the word similarity and analogy tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e786600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l_en.pytorch.d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a378920c",
   "metadata": {},
   "source": [
    "Below lists pretrained `GloVe` embeddings of dimension 50, 100, and 300, which can be downloaded from the [GloVe website](https://nlp.stanford.edu/projects/glove/). The pretrained fastText embeddings are available in multiple languages. Here we consider one English version (300-dimensional “wiki.en”) that can be downloaded from the [fastText website](https://fasttext.cc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee2208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n",
    "                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n",
    "                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n",
    "                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n",
    "                           'c1816da3821ae9f43899be655002f6c723e91b88')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ab3eb",
   "metadata": {},
   "source": [
    "To load these pretrained `GloVe` and `fastText` embeddings, we define the following `TokenEmbedding` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a37af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx2token, self.idx2vec = self._load_embedding(embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token2idx = {token: idx for idx, token in\n",
    "                          enumerate(self.idx2token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx2token, idx2vec = [\"<unk>\"], []\n",
    "        dataDIr = d2l.download_extract(embedding_name)\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(dataDIr, \"vec.txt\"), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(ele) for ele in elems[1:]]\n",
    "                # skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx2token.append(token)\n",
    "                    idx2vec.append(elems)\n",
    "        idx2vec = [[0] * len(idx2vec[0])] + idx2vec\n",
    "        return idx2token, torch.tensor(idx2vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token2idx.get(token, self.unknown_idx) for token in tokens]\n",
    "        vecs = self.idx2vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637c055",
   "metadata": {},
   "source": [
    "Below we load the 50-dimensional GloVe embeddings (pretrained on a Wikipedia subset). When creating the ` TokenEmbedding` instance, the specified embedding file has to be downloaded if it was not yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0c5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b50d = TokenEmbedding('glove.6b.50d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f1112e",
   "metadata": {},
   "source": [
    "Output the vocabulary size. The vocabulary contains 400000 words (tokens) and a special unknown token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44fc972f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eeaada",
   "metadata": {},
   "source": [
    "We can get the index of a word in the vocabulary, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f97582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of the word 'beautiful' is :3367, and the word of index 123 is :day\n"
     ]
    }
   ],
   "source": [
    "print(f'the index of the word \\'beautiful\\' is :{glove_6b50d.token2idx[\"beautiful\"]}, '\n",
    "      f'and the word of index 123 is :{glove_6b50d.idx2token[123]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dadfa3",
   "metadata": {},
   "source": [
    "## 7.2. Applying Pretrained Word Vectors\n",
    "\n",
    "Using the loaded `GloVe` vectors, we will demonstrate their semantics by applying them in the following word similarity and analogy tasks.\n",
    "\n",
    "### 7.2.1. Word Similarity\n",
    "\n",
    "Similar to [Section 4.3](./4_Pretraining.ipynb), in order to find semantically similar words for an input word based on cosine similarities between word vectors, we implement the following ` knn ` (k-nearest neighbors) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "528ddb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(W, x, k):\n",
    "    # Add 1e-9 for numerical stability\n",
    "    cos = torch.mv(W, x.reshape(-1, ))/(\n",
    "        torch.sqrt(torch.sum(W*W, axis=1) + 1e-9) *\n",
    "        torch.sqrt((x*x).sum())\n",
    "    )\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    return topk, [cos[int(i)] for i in topk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf9a38",
   "metadata": {},
   "source": [
    "Then, we search for similar words using the pretrained word vectors from the ` TokenEmbedding ` instance `embed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f37a404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed:TokenEmbedding):\n",
    "    topk, cos = knn(embed.idx2vec, embed[[query_token]], k+1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):\n",
    "        print(f'cosine sim = {float(c): .3f}: {embed.idx2token[int(i)]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ba68f",
   "metadata": {},
   "source": [
    "The vocabulary of the pretrained word vectors in `glove_6b50d` contains 400000 words and a special unknown token. Excluding the input word and unknown token, among this vocabulary let us find three most semantically similar words to word “chip”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "060b8df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim =  0.856: chips\n",
      "cosine sim =  0.749: intel\n",
      "cosine sim =  0.749: electronics\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('chip', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b6296",
   "metadata": {},
   "source": [
    "Below outputs similar words to “baby” and “beautiful”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d842409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim =  0.921: lovely\n",
      "cosine sim =  0.893: gorgeous\n",
      "cosine sim =  0.830: wonderful\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('beautiful', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a1dd6",
   "metadata": {},
   "source": [
    "### 7.2.2. Word Analogy\n",
    "\n",
    "Besides finding similar words, we can also apply **word vectors to word analogy tasks**. For example, “man”:“woman”::“son”:“daughter” is the form of a word analogy: “man” is to “woman” as “son” is to “daughter”. Specifically, the word analogy completion task can be defined as: for a word analogy a : b :: c : d, given the first three words a, b and c, find d. Denote the vector of word w by $vec(w)$. To complete the analogy, we will find the word whose vector is most similar to the result of $vec(c)+vec(b)−vec(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7a00fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c, embed:TokenEmbedding):\n",
    "    \"\"\" a: b ~ c : ?(d), return d\n",
    "    \n",
    "    :param token_a: \n",
    "    :param token_b: \n",
    "    :param token_c: \n",
    "    :param embed: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    vecs = embed[[token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs [0] + vecs[2]\n",
    "    topk, cos = knn(embed.idx2vec, x, 1)\n",
    "    return embed.idx2token[int(topk[0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09c4ce",
   "metadata": {},
   "source": [
    "Let us verify the “male-female” analogy using the loaded word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e7b830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('man', 'woman', 'son', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93855f34",
   "metadata": {},
   "source": [
    "Below completes a “capital-country” analogy: “beijing”:“china”::“tokyo”:“japan”. This demonstrates semantics in the pretrained word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c55441f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biggest'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('bad', 'worst', 'big', glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78a448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
