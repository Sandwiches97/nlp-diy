# Language Models and the Dataset

:label:`sec_language_model`

- **è¯­è¨€æ¨¡å‹**æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„å…³é”®ã€‚
- **n-grams** é€šè¿‡æˆªæ–­ç›¸å…³æ€§ï¼Œä¸ºå¤„ç†é•¿åºåˆ—æä¾›äº†ä¸€ç§å®ç”¨çš„æ¨¡å‹ã€‚
- **é•¿åºåˆ—**å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šå®ƒä»¬å¾ˆå°‘å‡ºç°æˆ–è€…ä»ä¸å‡ºç°ã€‚
- Zipf-law æ”¯é…ç€å•è¯çš„åˆ†å¸ƒï¼Œè¿™ä¸ªåˆ†å¸ƒä¸ä»…é€‚ç”¨äºä¸€å…ƒè¯­æ³•(unigrams)ï¼Œè¿˜é€‚ç”¨äºå…¶ä»–å…ƒè¯­æ³•(n-grams)ã€‚
- é€šè¿‡**æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æ³•**å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ç»“æ„ä¸°å¯Œè€Œé¢‘ç‡ä¸è¶³çš„ä½é¢‘è¯è¯ç»„ã€‚
- è¯»å–**é•¿åºåˆ—**çš„ä¸»è¦æ–¹å¼æ˜¯**éšæœºé‡‡æ ·ï¼ˆRandom Samplingï¼‰**å’Œ**é¡ºåºåˆ†åŒºï¼ˆSequential Patitioningï¼‰**ã€‚åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œåè€…å¯ä»¥ä¿è¯æ¥è‡ªä¸¤ä¸ªç›¸é‚»çš„å°æ‰¹é‡ä¸­çš„å­åºåˆ—åœ¨åŸå§‹åºåˆ—ä¸Šä¹Ÿæ˜¯ç›¸é‚»çš„ã€‚

In [section 3](3â€”â€”), we see how to map text data into tokens, where these tokens can be viewed as a sequence of discrete observations, such as words or characters. Assume that the tokens in a text sequence of length $T$ are in turn $x_1, x_2, \ldots, x_T$. Then, in the text sequence, $x_t$($1 \leq t \leq T$) can be considered as the observation or label at time step $t$. Given such a text sequence, the goal of a **language model** is to estimate the joint probability of the sequence

$$
P(x_1, x_2, \ldots, x_T).

$$

$\color{red}\text{Language models}$ are incredibly useful. **For instance**, an ideal language model would be able to generate natural text just on its own, simply by drawing one token at a time $x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)$. **Quite unlike** the monkey using a typewriter, all text emerging from such a model would pass as natural language, e.g., English text. **Furthermore**, it would be sufficient for generating a meaningful dialog, simply by conditioning the text on previous dialog fragments. Clearly we are still very far from designing such a system, since it would need to *understand* the text rather than just generate grammatically sensible content.

å°½ç®¡å¦‚æ­¤ï¼Œè¯­è¨€æ¨¡å‹ä¾ç„¶æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚ ä¾‹å¦‚ï¼ŒçŸ­è¯­â€œto recognize speechâ€å’Œâ€œto wreck a nice beachâ€è¯»éŸ³ä¸Šå¬èµ·æ¥éå¸¸ç›¸ä¼¼ã€‚ è¿™ç§ç›¸ä¼¼æ€§ä¼šå¯¼è‡´è¯­éŸ³è¯†åˆ«ä¸­çš„æ­§ä¹‰ï¼Œä½†æ˜¯è¿™å¾ˆå®¹æ˜“é€šè¿‡è¯­è¨€æ¨¡å‹æ¥è§£å†³ï¼Œ å› ä¸ºç¬¬äºŒå¥çš„è¯­ä¹‰å¾ˆå¥‡æ€ªã€‚ åŒæ ·ï¼Œåœ¨æ–‡æ¡£æ‘˜è¦ç”Ÿæˆç®—æ³•ä¸­ï¼Œ â€œç‹—å’¬äººâ€æ¯”â€œäººå’¬ç‹—â€å‡ºç°çš„é¢‘ç‡è¦é«˜å¾—å¤šï¼Œ æˆ–è€…â€œæˆ‘æƒ³åƒå¥¶å¥¶â€æ˜¯ä¸€ä¸ªç›¸å½“åŒªå¤·æ‰€æ€çš„è¯­å¥ï¼Œ è€Œâ€œæˆ‘æƒ³åƒï¼Œå¥¶å¥¶â€åˆ™è¦æ­£å¸¸å¾—å¤šã€‚

## 3.1. Learning a Language Model

æ˜¾è€Œæ˜“è§ï¼Œæˆ‘ä»¬é¢å¯¹çš„é—®é¢˜æ˜¯å¦‚ä½•å¯¹ä¸€ä¸ª Documentï¼Œ ç”šè‡³æ˜¯ä¸€ä¸ªè¯å…ƒåºåˆ— (sequence) è¿›è¡Œå»ºæ¨¡ã€‚ Suppose that we tokenize text data **at the word level**. We can take recourse to the analysis we applied to sequence models in [section 1](). Let us start by applying basic probability rules:

$$
P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1}).

$$

**For example**,the probability of **a text sequence** containing four words would be given as:

$$
P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is}).

$$

In order to compute the $\color{red}\text{Language model}$, we need to calculate the probability of words and the conditional probability of a word given the previous few words. **Such probabilities are essentially language model parameters.**  æ¦‚ç‡æœ¬è´¨å°±æ˜¯ $\color{red}\text{Language model}$ çš„å‚æ•°

Here, we assume that the training dataset is a large text corpus, such as all Wikipedia entries, [Project Gutenberg](https://en.wikipedia.org/wiki/Project_Gutenberg), and all text posted on the Web. è®­ç»ƒæ•°æ®é›†ä¸­è¯çš„æ¦‚ç‡å¯ä»¥æ ¹æ® a given word çš„ **relative word frequency** æ¥è®¡ç®—. **For example**, the estimate  $\hat{P}(deep)$ can be calculated as the probability of any sentence starting with the word "deep".

- ä¸€ç§ï¼ˆç¨ç¨ä¸å¤ªç²¾ç¡®çš„ï¼‰æ–¹æ³•æ˜¯ç»Ÿè®¡å•è¯â€œdeepâ€åœ¨æ•°æ®é›†ä¸­çš„å‡ºç°æ¬¡æ•°ï¼Œ ç„¶åå°†å…¶é™¤ä»¥æ•´ä¸ªè¯­æ–™åº“ä¸­çš„å•è¯æ€»æ•°ã€‚è¿™ç§æ–¹æ³•æ•ˆæœä¸é”™ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé¢‘ç¹å‡ºç°çš„å•è¯ã€‚
- æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¼°è®¡

$$
\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})},

$$

where $n(x)$ and $n(x, x')$ are the number of occurrences of singletons and consecutive word pairs, respectively.

$\text{\colorbox{black}{\color{yellow}Unfortunately}}$, ç”±äºè¿ç»­å•è¯å¯¹â€œdeep learningâ€çš„å‡ºç°é¢‘ç‡è¦ä½å¾—å¤šï¼Œ æ‰€ä»¥ä¼°è®¡è¿™ç±»å•è¯æ­£ç¡®çš„æ¦‚ç‡è¦å›°éš¾å¾—å¤šã€‚ **In particular**, for some unusual word combinations it may be tricky to find enough occurrences to get accurate estimates. Things take a turn for the worse for three-word combinations and beyond. There will be many plausible three-word combinations that we likely will not see in our dataset. Unless we provide some solution to assign such word combinations nonzero count, we will not be able to use them in a language model. If the dataset is small or if the words are very rare, we might not find even a single one of them.

A common strategy is to perform some form of  $\text{\colorbox{black}{\color{red}Laplace smoothing}}$ . The solution is to add a small constant to all counts. Denote by $ğ‘›$ the total number of words in the training set and **ğ‘š**m the number of unique words. This solution helps with singletons, e.g., via

$$
\begin{aligned}
	\hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\
	\hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \\
	\hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.
\end{aligned}

$$

Here $\epsilon_1,\epsilon_2$, and $\epsilon_3$ are hyperparameters. Take $\epsilon_1$ as an example: when $\epsilon_1 = 0$, no smoothing is applied; when $\epsilon_1$ approaches positive infinity, $\hat{P}(x)$ approaches the uniform probability $1/m$. The above is a rather primitive variant of what other techniques can accomplish [[Wood et al., 2011]](https://d2l.ai/chapter_references/zreferences.html#wood-gasthaus-archambeau-ea-2011).

**Unfortunately**, è¿™æ ·çš„æ¨¡å‹å¾ˆå®¹æ˜“å˜å¾—æ— æ•ˆ:

- First, we need to store all counts.
- Second, this entirely $\text{\colorbox{black}{\color{magenta}ignores the meaning of the words}}$. **For instance**, "cat" and "feline" should occur in related contexts. It is quite difficult to adjust such models to additional contexts, whereas, deep learning based language models are well suited to take this into account.
- Last, long word sequences are almost certain to be novel, (é•¿å•è¯åºåˆ—å¤§éƒ¨åˆ†éƒ½æ˜¯æœªå‡ºç°è¿‡çš„)

hence a model that simply counts the frequency of previously seen word sequences is bound to perform poorly there.

## 3.2. Markov Models and $n$-grams ï¼ˆnå…ƒè¯­æ³•ï¼‰

Before we discuss solutions involving deep learning, we need some more **terminology and concepts**. Recall our discussion of $\text{\colorbox{black}{\color{red}Markov Models}}$ in [Section 8.1](1_Sequence_Models.md). Let us apply this to language modeling. A distribution over sequences satisfies the $\text{\colorbox{black}{\color{red}Markov property of first order}}$ if $P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t)$. é˜¶æ•°è¶Šé«˜ï¼Œå¯¹åº”çš„ä¾èµ–å…³ç³»å°±è¶Šé•¿ã€‚ This leads to a number of approximations that we could apply to model a sequence:

$$
\begin{aligned}
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4), \text{ä¸€å…ƒè¯­æ³•ï¼Œç»Ÿè®¡è¯é¢‘}\\
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\\
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).
\end{aligned}

$$

é€šå¸¸ï¼Œæ¶‰åŠä¸€ä¸ªã€ä¸¤ä¸ªå’Œä¸‰ä¸ªå˜é‡çš„æ¦‚ç‡å…¬å¼åˆ†åˆ«è¢«ç§°ä¸º â€œä¸€å…ƒè¯­æ³•â€ï¼ˆunigramï¼‰ã€â€œäºŒå…ƒè¯­æ³•â€ï¼ˆbigramï¼‰å’Œâ€œä¸‰å…ƒè¯­æ³•â€ï¼ˆtrigramï¼‰æ¨¡å‹ã€‚ ä¸‹é¢ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å»è®¾è®¡æ›´å¥½çš„æ¨¡å‹ã€‚

## 3.3. Natural Language Statistics

Let us see how this works on real data. We construct a vocabulary based on the time machine dataset as introduced in [Section 8.2](2_Text_Preprocessing.md) and print the top 10 most frequent words.

```python
import random
import torch
from d2l import torch as d2l
```

```python
tokens = d2l.tokenize(d2l.read_time_machine())
# å› ä¸ºæ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œå› æ­¤æˆ‘ä»¬æŠŠæ‰€æœ‰æ–‡æœ¬è¡Œæ‹¼æ¥åˆ°ä¸€èµ·
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
vocab.token_freqs[:10]
```

```
[('the', 2261),
('i', 1267),
('and', 1245),
('of', 1155),
('a', 816),
('to', 695),
('was', 552),
('in', 541),
('that', 443),
('my', 440)]
```

As we can see, (**the most popular words are** æ²¡ä»€ä¹ˆç”¨) actually quite boring to look at. They are often referred to as (***stop words***) and thus filtered out ï¼ˆä¸€èˆ¬ä¼šè¢«è¿‡æ»¤æ‰ï¼‰.  **Nonetheless**, å®ƒä»¬æœ¬èº«ä»ç„¶æ˜¯æœ‰æ„ä¹‰çš„ï¼Œæˆ‘ä»¬ä»ç„¶ä¼šåœ¨æ¨¡å‹ä¸­ä½¿ç”¨å®ƒä»¬. **Besides**, it is quite clear that the word frequency decays rather rapidly ï¼ˆè¡°å‡å¾ˆå¿«ï¼‰. The $10^{\mathrm{th}}$ most frequent word is less than $1/5$ as common as the most popular one. To get a better idea, we [**plot the figure of the word frequency**].

```python
freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')
```

![../_images/output_language-models-and-dataset_789d14_18_0.svg](https://zh.d2l.ai/_images/output_language-models-and-dataset_789d14_18_0.svg)

é€šè¿‡æ­¤å›¾æˆ‘ä»¬å¯ä»¥å‘ç°ï¼šè¯é¢‘ä»¥ä¸€ç§æ˜ç¡®çš„æ–¹å¼è¿…é€Ÿè¡°å‡ã€‚ å°†å‰å‡ ä¸ªå•è¯ä½œä¸ºä¾‹å¤–æ¶ˆé™¤åï¼Œå‰©ä½™çš„æ‰€æœ‰å•è¯å¤§è‡´éµå¾ªåŒå¯¹æ•°åæ ‡å›¾ä¸Šçš„ä¸€æ¡ç›´çº¿ã€‚ This means that words satisfy *Zipf's law*, which states that the frequency $n_i$ of the $i^\mathrm{th}$ most frequent word is:

$$
n_i \propto \frac{1}{i^\alpha},\tag{8.3.7}

$$

which is equivalent to

$$
\log n_i = -\alpha \log i + c,\tag{8.3.8}

$$

where $\alpha$ is the exponent that characterizes the distribution and $c$ is a constant. è¿™å‘Šè¯‰æˆ‘ä»¬æƒ³è¦é€šè¿‡è®¡æ•°ç»Ÿè®¡å’Œå¹³æ»‘æ¥å»ºæ¨¡å•è¯æ˜¯ä¸å¯è¡Œçš„ï¼Œ å› ä¸ºè¿™æ ·å»ºæ¨¡çš„ç»“æœä¼šå¤§å¤§é«˜ä¼°å°¾éƒ¨å•è¯çš„é¢‘ç‡ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„ä¸å¸¸ç”¨å•è¯ã€‚

But [**what about the other word combinations, such as bigrams, trigrams**], and beyond?

Let us see whether the **bigram** frequency behaves in the same manner as the unigram frequency.

```python
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
bigram_vocab = d2l.Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]
```

```
[(('of', 'the'), 309),
(('in', 'the'), 169),
(('i', 'had'), 130),
(('i', 'was'), 112),
(('and', 'the'), 109),
(('the', 'time'), 102),
(('it', 'was'), 99),
(('to', 'the'), 85),
(('as', 'i'), 78),
(('of', 'a'), 73)]
```

è¿™é‡Œå€¼å¾—æ³¨æ„ï¼šåœ¨åä¸ªæœ€é¢‘ç¹çš„è¯å¯¹ä¸­ï¼Œæœ‰ä¹ä¸ªæ˜¯ç”±ä¸¤ä¸ªåœç”¨è¯ç»„æˆçš„ï¼Œ åªæœ‰ä¸€ä¸ªä¸â€œthe timeâ€æœ‰å…³ã€‚ æˆ‘ä»¬å†è¿›ä¸€æ­¥çœ‹çœ‹ trigrams çš„é¢‘ç‡æ˜¯å¦è¡¨ç°å‡ºç›¸åŒçš„è¡Œä¸ºæ–¹å¼ã€‚

```python
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]
```

```
[(('the', 'time', 'traveller'), 59),
(('the', 'time', 'machine'), 30),
(('the', 'medical', 'man'), 24),
(('it', 'seemed', 'to'), 16),
(('it', 'was', 'a'), 15),
(('here', 'and', 'there'), 15),
(('seemed', 'to', 'me'), 14),
(('i', 'did', 'not'), 14),
(('i', 'saw', 'the'), 13),
(('i', 'began', 'to'), 13)]
```

æœ€åï¼Œæˆ‘ä»¬ç›´è§‚åœ°å¯¹æ¯”ä¸‰ç§æ¨¡å‹ä¸­çš„è¯å…ƒé¢‘ç‡ï¼šä¸€å…ƒè¯­æ³•ã€äºŒå…ƒè¯­æ³•å’Œä¸‰å…ƒè¯­æ³•ã€‚

```python
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```

![../_images/output_language-models-and-dataset_789d14_54_0.svg](https://zh.d2l.ai/_images/output_language-models-and-dataset_789d14_54_0.svg)

This figure is quite exciting for a number of reasons.

- First, beyond unigram words, **sequences of words**(å•è¯åºåˆ—ä¹Ÿéµå¾ª Zipf å®šå¾‹) also appear to be following Zipf's law, albeit with a smaller exponent $\alpha$ in [(8.3.7)](), depending on the sequence length.
- Second, è¯è¡¨ä¸­ n å…ƒç»„çš„æ•°é‡å¹¶æ²¡æœ‰é‚£ä¹ˆå¤§ï¼Œè¿™è¯´æ˜è¯­è¨€ä¸­å­˜åœ¨ç›¸å½“å¤šçš„ç»“æ„
- Third, å¾ˆå¤š n å…ƒç»„å¾ˆå°‘å‡ºç°ï¼Œè¿™ä½¿å¾—æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘éå¸¸ä¸é€‚åˆè¯­è¨€å»ºæ¨¡

Instead, we will use **deep learning based models**.

## 3.4. Reading Long Sequence Data è¯»å–é•¿åºåˆ—

Since sequence data are by their very nature sequential, we need to address the issue of processing it. We did so in a rather ad-hoc (å¾ˆç‰¹åˆ«çš„æ–¹å¼) manner [Section 8.1](1_Sequences_Models.md). When sequences get too long to be processed by models all at once, we may wish to split such sequences for reading. æ¨¡å‹å¤ªé•¿çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½æŠŠè¿™æ ·çš„åºåˆ—æ‹†åˆ†äº†ã€‚

åœ¨ä»‹ç»è¯¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸‹æ€»ä½“ç­–ç•¥ã€‚ å‡è®¾æˆ‘ä»¬å°†ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œ æ¨¡å‹ä¸­çš„ç½‘ç»œä¸€æ¬¡å¤„ç†å…·æœ‰é¢„å®šä¹‰é•¿åº¦ ï¼ˆä¾‹å¦‚ $n$ ä¸ªæ—¶é—´æ­¥ï¼‰çš„ä¸€ä¸ªå°æ‰¹é‡åºåˆ—ã€‚ ç°åœ¨çš„é—®é¢˜æ˜¯å¦‚ä½•éšæœºç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡æ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ä»¥ä¾›è¯»å–ã€‚

é¦–å…ˆï¼Œç”±äºæ–‡æœ¬åºåˆ—å¯ä»¥æ˜¯ä»»æ„é•¿çš„ï¼Œ ä¾‹å¦‚æ•´æœ¬ã€Šæ—¶å…‰æœºå™¨ã€‹ï¼ˆ *The Time Machine* ï¼‰, we can **partition** such a long sequence **into** `subsequences` with the same number of time steps. When training our neural network, a minibatch of such subsequences will be fed into the model.

Suppose that the network processes a subsequence of $n$ time steps at a time. [Fig. 8.1.3]() shows all the different ways to obtain subsequences from an original text sequence, where $n=5$ and a token at each time step corresponds to a character. è¯·æ³¨æ„ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä»»æ„åç§»é‡ (offset) æ¥æŒ‡ç¤ºåˆå§‹ä½ç½®ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰ç›¸å½“å¤§çš„è‡ªç”±åº¦ã€‚

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://d2l.ai/_images/timemachine-5gram.svg" width = "50%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      Fig. 8.1.3 Different offsets lead to different subsequences when splitting up text.
  	</div>
</center>

Hence, which one should we pick from [Fig. 8.3.1](https://d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html#fig-timemachine-5gram)?

äº‹å®ä¸Šï¼Œä»–ä»¬éƒ½ä¸€æ ·çš„å¥½ã€‚ ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬åªé€‰æ‹©ä¸€ä¸ªåç§»é‡ï¼Œ é‚£ä¹ˆç”¨äºè®­ç»ƒç½‘ç»œçš„ã€æ‰€æœ‰å¯èƒ½çš„å­åºåˆ—çš„è¦†ç›–èŒƒå›´å°†æ˜¯æœ‰é™çš„ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä»éšæœºåç§»é‡ (a random offset) å¼€å§‹åˆ’åˆ†åºåˆ—ï¼Œ ä»¥åŒæ—¶è·å¾— *è¦†ç›–æ€§* ï¼ˆcoverageï¼‰å’Œ *éšæœºæ€§* ï¼ˆrandomnessï¼‰ã€‚ ä¸‹é¢ï¼Œæˆ‘ä»¬å°†æè¿°å¦‚ä½•å®ç°

- *éšæœºé‡‡æ ·* ï¼ˆrandom samplingï¼‰å’Œ
- *é¡ºåºåˆ†åŒº* ï¼ˆsequential partitioningï¼‰ç­–ç•¥ã€‚

### 3.4.1Random Sampling éšæœºé‡‡æ ·

( **In $\text{\colorbox{white}{\color{red}random sampling}}$, each example is a subsequence arbitrarily captured on the original long sequence.** ) The subsequences from two adjacent random minibatches during iteration are not necessarily adjacent on the original sequence. (å°±æ˜¯å°†batch size**å†…éƒ¨**æ‰“åŒ…çš„æ—¶å€™ï¼Œä¸è¦æŒ‰é¡ºåºæ‰“åŒ…ï¼Œè€Œæ˜¯éšæœºæ‰“åŒ…)

For **$\text{\colorbox{black}{\color{red}language modeling}}$**, the target is to $\text{\colorbox{black}{\color{yellow}predict the next token}}$ based on what tokens we have seen so far, hence the `labels` are the original sequence, **$\text{\colorbox{black}{\color{yellow}shifted by one token}}$**.

ä¸‹é¢çš„ä»£ç æ¯æ¬¡å¯ä»¥ä»æ•°æ®ä¸­éšæœºç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡ã€‚ Here, the argument

- `batch_size` specifies the number of subsequence examples in each minibatch and
- `num_steps` is the predefined number of time steps in each subsequence.

```python
def seq_data_iter_random(corpus, batch_size, num_steps)->Tuple:  #@save
    """ ä¸‹é¢çš„ä»£ç æ¯æ¬¡å¯ä»¥ä»æ•°æ®ä¸­éšæœºç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡ã€‚

    :param corpus: a list
    :param batch_size: æ¯ä¸ªå°æ‰¹é‡ä¸­å­åºåˆ—æ ·æœ¬çš„æ•°ç›®
    :param num_steps: æ¯ä¸ªå­åºåˆ—ä¸­é¢„å®šä¹‰çš„æ—¶é—´æ­¥æ•°
    :return:
    """
    # ä»éšæœºåç§»é‡å¼€å§‹å¯¹åºåˆ—è¿›è¡Œåˆ†åŒºï¼ŒéšæœºèŒƒå›´åŒ…æ‹¬num_steps-1
    corpus = corpus[random.randint(0, num_steps - 1):]
    # å‡å»1ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦è€ƒè™‘æ ‡ç­¾, idx_Y = (idx_X + 1)
    num_subseqs = (len(corpus) - 1) // num_steps        # å­åºåˆ—çš„ä¸ªæ•°
    # The starting indices èµ·å§‹ç´¢å¼• for subsequences of length `num_steps`
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # åœ¨éšæœºæŠ½æ ·çš„è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œ
    # æ¥è‡ªä¸¤ä¸ªç›¸é‚»çš„ã€éšæœºçš„ã€å°æ‰¹é‡ä¸­çš„å­åºåˆ—ä¸ä¸€å®šåœ¨åŸå§‹åºåˆ—ä¸Šç›¸é‚»
    random.shuffle(initial_indices)

    def data(pos):
        # è¿”å›ä»posä½ç½®å¼€å§‹çš„é•¿åº¦ä¸ºnum_stepsçš„åºåˆ—
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # åœ¨è¿™é‡Œï¼Œinitial_indices åŒ…å«å­åºåˆ—çš„éšæœºèµ·å§‹ç´¢å¼•
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)def seq_data_iter_random(corpus, batch_size, num_steps):  #@save
    """Generate a minibatch of subsequences using random sampling."""
    # Start with a random offset (inclusive of `num_steps - 1`) to partition a
    # sequence
    corpus = corpus[random.randint(0, num_steps - 1):]
    # Subtract 1 since we need to account for labels
    num_subseqs = (len(corpus) - 1) // num_steps
    # The starting indices for subsequences of length `num_steps`
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # In random sampling, the subsequences from two adjacent random
    # minibatches during iteration are not necessarily adjacent on the
    # original sequence
    random.shuffle(initial_indices)

    def data(pos):
        # Return a sequence of length `num_steps` starting from `pos`
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # Here, `initial_indices` contains randomized starting indices for
        # subsequences
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)
```

Let us [**manually generate a sequence from 0 to 34.**] We assume that the `batch size` and `numbers of time steps` are 2 and 5, respectively. This means that we can generate $\lfloor (35 - 1) / 5 \rfloor= 6$ `feature-label subsequence pairs`. With a minibatch size of 2, we only get 3 minibatches.

```python
my_seq = list(range(35))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

```
X:  tensor([[24, 25, 26, 27, 28],
[ 9, 10, 11, 12, 13]])
Y: tensor([[25, 26, 27, 28, 29],
[10, 11, 12, 13, 14]])
X:  tensor([[29, 30, 31, 32, 33],
[ 4,  5,  6,  7,  8]])
Y: tensor([[30, 31, 32, 33, 34],
[ 5,  6,  7,  8,  9]])
X:  tensor([[14, 15, 16, 17, 18],
[19, 20, 21, 22, 23]])
Y: tensor([[15, 16, 17, 18, 19],
[20, 21, 22, 23, 24]])
```

### Sequential Partitioning é¡ºåºåˆ†åŒº

In addition to random sampling of the original sequence, [ æˆ‘ä»¬è¿˜å¯ä»¥ä¿è¯ä¸¤ä¸ªç›¸é‚»çš„å°æ‰¹é‡ä¸­çš„å­åºåˆ—åœ¨åŸå§‹åºåˆ—ä¸Šä¹Ÿæ˜¯ç›¸é‚»çš„ ] ã€‚ è¿™ç§ç­–ç•¥åœ¨åŸºäºå°æ‰¹é‡çš„è¿­ä»£è¿‡ç¨‹ä¸­ä¿ç•™äº†æ‹†åˆ†çš„å­åºåˆ—çš„é¡ºåº, hence is called $\text{\colorbox{white}{\color{red}sequential partitioning}}$.

```python
def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save
    """Generate a minibatch of subsequences using sequential partitioning."""
    # Start with a random offset to partition a sequence
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
```

Using the same settings, let us [ **print features `X` and labels `Y` for each minibatch** ] of subsequences read by sequential partitioning. é€šè¿‡å°†å®ƒä»¬æ‰“å°å‡ºæ¥å¯ä»¥å‘ç°ï¼š è¿­ä»£æœŸé—´æ¥è‡ªä¸¤ä¸ªç›¸é‚»çš„å°æ‰¹é‡ä¸­çš„å­åºåˆ—åœ¨åŸå§‹åºåˆ—ä¸­ç¡®å®æ˜¯ç›¸é‚»çš„ã€‚

```python
for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

```
X:  tensor([[ 0,  1,  2,  3,  4],
[17, 18, 19, 20, 21]])
Y: tensor([[ 1,  2,  3,  4,  5],
[18, 19, 20, 21, 22]])
X:  tensor([[ 5,  6,  7,  8,  9],
[22, 23, 24, 25, 26]])
Y: tensor([[ 6,  7,  8,  9, 10],
[23, 24, 25, 26, 27]])
X:  tensor([[10, 11, 12, 13, 14],
[27, 28, 29, 30, 31]])
Y: tensor([[11, 12, 13, 14, 15],
[28, 29, 30, 31, 32]])
```

Now we wrap the above two sampling functions to a `class`

```python
class SeqDataLoader:  #@save
    """An iterator to load sequence data."""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
```

[**Last, we define a function `load_data_time_machine` that returns both the data iterator and the vocabulary**], so we can use it similarly as other other functions with the `load_data` prefix, such as `d2l.load_data_fashion_mnist` defined [Section 3.5](https://d2l.ai/chapter_linear-networks/image-classification-dataset.html#sec-fashion-mnist).

```python
def load_data_time_machine(batch_size, num_steps,  #@save
                           use_random_iter=False, max_tokens=10000):
    """Return the iterator and the vocabulary of the time machine dataset."""
    data_iter = SeqDataLoader(
        batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
```

## Summary

* Language models are key to natural language processing.
* $n$-grams provide a convenient model for dealing with long sequences by truncating the dependence.
* Long sequences suffer from the problem that they occur very rarely or never.
* Zipf's law governs the word distribution for not only unigrams but also the other $n$-grams.
* There is a lot of structure but not enough frequency to deal with infrequent word combinations efficiently via Laplace smoothing.
* The main choices for reading long sequences are random sampling and sequential partitioning. The latter can ensure that the subsequences from two adjacent minibatches during iteration are adjacent on the original sequence.

## Exercises

1. Suppose there are $100,000$ words in the training dataset. How much word frequency and multi-word adjacent frequency does a four-gram need to store?
2. How would you model a dialogue?
3. Estimate the exponent of Zipf's law for unigrams, bigrams, and trigrams.
4. What other methods can you think of for reading long sequence data?
5. Consider the random offset that we use for reading long sequences.
   1. Why is it a good idea to have a random offset?
   2. Does it really lead to a perfectly uniform distribution over the sequences on the document?
   3. What would you have to do to make things even more uniform?
6. If we want a sequence example to be a complete sentence, what kind of problem does this introduce in minibatch sampling? How can we fix the problem?

[Discussions](https://discuss.d2l.ai/t/118)
