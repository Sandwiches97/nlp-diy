# 6.1. 从全连接层到卷积


我们之前讨论的多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。

例如，在之前猫狗分类的例子中：假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。 即使将隐藏层维度降低到1000，这个全连接层也将有 $10^6×10^3=10^9$个参数。 想要训练这个模型将不可实现，因为需要有大量的GPU、分布式优化训练的经验和超乎常人的耐心。

有些读者可能会反对这个观点，认为要求百万像素的分辨率可能不是必要的。 然而，即使分辨率减小为十万像素，使用1000个隐藏单元的隐藏层也可能不足以学习到良好的图像特征，在真实的系统中我们仍然需要数十亿个参数。 此外，拟合如此多的参数还需要收集大量的数据。 然而，如今人类和机器都能很好地区分猫和狗：这是因为图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。  **卷积神经网络** （convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法。



## 6.1.1. 不变性

想象一下，假设你想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。 理想情况下，我们的系统应该能够利用常识：猪通常不在天上飞，飞机通常不在水里游泳。 但是，如果一只猪出现在图片顶部，我们还是应该认出它。 我们可以从儿童游戏”沃尔多在哪里”（ [图6.1.1](https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#img-waldo)）中得到灵感： 在这个游戏中包含了许多充斥着活动的混乱场景，而沃尔多通常潜伏在一些不太可能的位置，读者的目标就是找出他。 尽管沃尔多的装扮很有特点，但是在眼花缭乱的场景中找到他也如大海捞针。 然而沃尔多的样子并不取决于他潜藏的地方，因此我们可以使用一个“沃尔多检测器”扫描图像。 该检测器将图像分割成多个区域，并为每个区域包含沃尔多的可能性打分。 卷积神经网络正是将 *空间不变性* （spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。



现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构：

1. **平移不变性** （translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
2. **局部性** （locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

让我们看看这些原则是如何转化为数学表示的。



## 6.1.2. 多层感知机的限制

首先，多层感知机的输入是二维图像 $X$，其隐藏表示 $H$ 在数学上是一个矩阵，在代码中表示为二维张量。 其中 $X$ 和 $H$ 具有相同的形状。 为了方便理解，我们可以认为，无论是输入还是隐藏表示都拥有空间结构。

使用 $[X]_{i,j}$ 和 $[H]_{i,j}$ 分别表示输入图像和隐藏表示中位置$（i,j）$处的像素。 为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵（如同我们先前在多层感知机中所做的那样）替换为四阶权重张量 $W$。假设 $U$ 包含偏置参数，我们可以将全连接层形式化地表示为
